{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"Reads a CSV file and returns the data as a pandas dataframe.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevent_data_leak(df):\n",
    "    \"\"\"- Divides the original dataset into train and validation sets to validate the model and prevent data leakage.\n",
    "    - Saves the split dataset into CSV format.\"\"\"\n",
    "    train, validation = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    validation.to_csv(\"house_price_validation.csv\", index=False)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_id(df):\n",
    "    \"\"\"Removes the alphanumeric feature 'Id' from the dataset.\"\"\"\n",
    "    df = df.drop(columns='Id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(df):\n",
    "    \"\"\"Eliminates outliers based on specific conditions:\n",
    "    - Properties with 'GrLivArea' greater than 4,000 sqft.\n",
    "    - 'SalePrice' exceeding 700,000.\n",
    "    - 'LotArea' surpassing 100,000 sqft.\n",
    "    - 'LotFrontage' above 300 sqft.\"\"\"\n",
    "\n",
    "    outliers = df[(df['GrLivArea']>4000)|(df['SalePrice']>700000)|(df['LotArea']>100000)|(df['LotFrontage']>300)].index\n",
    "    df.drop(index=outliers, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features_tr(df):\n",
    "    \"\"\"This function drops the features that are missing more than half of the values.\"\"\"\n",
    "    missing_values_dict={}\n",
    "    for column in df.columns:\n",
    "        percentage_missing = (df[column].isna().sum()/len(df[column]))*100\n",
    "        if percentage_missing > 50:\n",
    "            missing_values_dict[column] = percentage_missing\n",
    "            features = pd.Series(missing_values_dict).sort_values(ascending=True)\n",
    "            features_index = features.index\n",
    "\n",
    "    df.drop(columns=features_index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df, features_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features_ts(df, drop_ft_obj):\n",
    "    \"\"\"This function drops the features.\"\"\"\n",
    "    df.drop(columns=drop_ft_obj, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_imputation_tr(df):\n",
    "    \"\"\"Handles data imputation for train data, managing missing values within the dataset.\n",
    "    Also, returns dictionaries containing median values for:\n",
    "    - Median masonary veneer area\n",
    "    - Median lot frontage area\"\"\"\n",
    "\n",
    "    df['BsmtQual'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtQual'].isna())), 'N/A', df['BsmtQual'])\n",
    "    df['BsmtCond'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtCond'].isna())), 'N/A', df['BsmtCond'])\n",
    "    df['BsmtExposure'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtExposure'].isna())), 'N/A', df['BsmtExposure'])\n",
    "    df['BsmtFinType1'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtFinType1'].isna())), 'N/A', df['BsmtFinType1'])\n",
    "    df['BsmtFinType2'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtFinType2'].isna())), 'N/A', df['BsmtFinType2'])\n",
    "    df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0], inplace=True)\n",
    "\n",
    "    df['GarageType'] = np.where(((df['GarageArea']==0)&(df['GarageType'].isna())), 'N/A', df['GarageType'])\n",
    "    df['GarageYrBlt'] = np.where(((df['GarageArea']==0)&(df['GarageYrBlt'].isna())), 0, df['GarageYrBlt'])\n",
    "    df['GarageFinish'] = np.where(((df['GarageArea']==0)&(df['GarageFinish'].isna())), 'N/A', df['GarageFinish'])\n",
    "    df['GarageQual'] = np.where(((df['GarageArea']==0)&(df['GarageQual'].isna())), 'N/A', df['GarageQual'])\n",
    "    df['GarageCond'] = np.where(((df['GarageArea']==0)&(df['GarageCond'].isna())), 'N/A', df['GarageCond'])\n",
    "\n",
    "    df['Electrical'].fillna(df['Electrical'].mode()[0], inplace=True)\n",
    "    df['FireplaceQu'].fillna('N/A', inplace=True)\n",
    "\n",
    "    median_masonary = df.groupby('Neighborhood')['MasVnrArea'].median()\n",
    "    median_msonary_dictionary = median_masonary.to_dict()\n",
    "\n",
    "    median_lotfront = df.groupby('Neighborhood')['LotFrontage'].median()\n",
    "    median_lotfront_dictionary = median_lotfront.to_dict()\n",
    "\n",
    "    df['MasVnrArea'] = df.apply(lambda row: median_msonary_dictionary.get(row['Neighborhood'], row['MasVnrArea']) if pd.isna(row['MasVnrArea']) else row['MasVnrArea'],axis=1)\n",
    "    df['LotFrontage'] = df.apply(lambda row: median_lotfront_dictionary.get(row['Neighborhood'], row['LotFrontage']) if pd.isna(row['LotFrontage']) else row['LotFrontage'], axis=1)\n",
    "    return df, median_msonary_dictionary, median_lotfront_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_imputation_ts(df, mm_obj, lf_obj):\n",
    "    \"\"\"Handles data imputation for test data, managing missing values within the dataset.\"\"\"\n",
    "\n",
    "    df['BsmtQual'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtQual'].isna())), 'N/A', df['BsmtQual'])\n",
    "    df['BsmtCond'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtCond'].isna())), 'N/A', df['BsmtCond'])\n",
    "    df['BsmtExposure'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtExposure'].isna())), 'N/A', df['BsmtExposure'])\n",
    "    df['BsmtFinType1'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtFinType1'].isna())), 'N/A', df['BsmtFinType1'])\n",
    "    df['BsmtFinType2'] = np.where(((df['TotalBsmtSF']==0)&(df['BsmtFinType2'].isna())), 'N/A', df['BsmtFinType2'])\n",
    "    df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0], inplace=True)\n",
    "\n",
    "    df['GarageType'] = np.where(((df['GarageArea']==0)&(df['GarageType'].isna())), 'N/A', df['GarageType'])\n",
    "    df['GarageYrBlt'] = np.where(((df['GarageArea']==0)&(df['GarageYrBlt'].isna())), 0, df['GarageYrBlt'])\n",
    "    df['GarageFinish'] = np.where(((df['GarageArea']==0)&(df['GarageFinish'].isna())), 'N/A', df['GarageFinish'])\n",
    "    df['GarageQual'] = np.where(((df['GarageArea']==0)&(df['GarageQual'].isna())), 'N/A', df['GarageQual'])\n",
    "    df['GarageCond'] = np.where(((df['GarageArea']==0)&(df['GarageCond'].isna())), 'N/A', df['GarageCond'])\n",
    "\n",
    "    df['Electrical'].fillna(df['Electrical'].mode()[0], inplace=True)\n",
    "    df['FireplaceQu'].fillna('N/A', inplace=True)\n",
    "\n",
    "    median_msonary_dictionary = mm_obj\n",
    "    median_lotfront_dictionary = lf_obj\n",
    "\n",
    "    df['MasVnrArea'] = df.apply(lambda row: median_msonary_dictionary.get(row['Neighborhood'], row['MasVnrArea']) if pd.isna(row['MasVnrArea']) else row['MasVnrArea'],axis=1)\n",
    "    df['LotFrontage'] = df.apply(lambda row: median_lotfront_dictionary.get(row['Neighborhood'], row['LotFrontage']) if pd.isna(row['LotFrontage']) else row['LotFrontage'], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_org(df):\n",
    "    \"\"\"Organizes the dataset by performing the following steps:\n",
    "    - Converts binary features with 'yes' or 'no' values into binary numeric format.\n",
    "    - Replaces categorical ordinal values with corresponding numeric representations.\n",
    "    - Creates a 'quality condition' feature by averaging quality and condition attributes.\n",
    "    - Aggregates basement finish types into a single feature for enhanced clarity.\"\"\"\n",
    "\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype('object')\n",
    "\n",
    "    df['GarageFinish'] = df['GarageFinish'].map(lambda x: 1 if x=='Fin' else 0)\n",
    "    df['CentralAir'] = df['CentralAir'].map(lambda x: 1 if x=='Y' else 0)\n",
    "    df['Functional'] = df['Functional'].map(lambda x: 1 if x=='Typ' else 0)\n",
    "    df['PavedDrive'] = df['PavedDrive'].map(lambda x: 1 if x=='Y' else 0)\n",
    "    df['Fireplaces'] = df['Fireplaces'].map(lambda x: 1 if x>0 else 0)\n",
    "    df['Street'] = df['Street'].map(lambda x: 1 if x=='Pave' else 0)\n",
    "    df['Utilities'] = df['Utilities'].map(lambda x: 1 if x=='AllPub' else 0)\n",
    "\n",
    "    qc_dictionary = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'N/A':0}\n",
    "    height_dictionary = {'Ex':100, 'Gd':90, 'TA':80, 'Fa':70, 'Po':60, 'N/A':0}\n",
    "\n",
    "    df['ExterQual'] = df['ExterQual'].replace(qc_dictionary)\n",
    "    df['ExterCond'] = df['ExterCond'].replace(qc_dictionary)\n",
    "    df['BsmtCond'] = df['BsmtCond'].replace(qc_dictionary)\n",
    "    df['HeatingQC'] = df['HeatingQC'].replace(qc_dictionary)\n",
    "    df['KitchenQual'] = df['KitchenQual'].replace(qc_dictionary)\n",
    "    df['FireplaceQu'] = df['FireplaceQu'].replace(qc_dictionary)\n",
    "    df['GarageQual'] = df['GarageQual'].replace(qc_dictionary)\n",
    "    df['GarageCond'] = df['GarageCond'].replace(qc_dictionary)\n",
    "\n",
    "    df['BsmtHeight'] = df['BsmtQual'].replace(height_dictionary)\n",
    "    df.drop(columns='BsmtQual', inplace=True)\n",
    "\n",
    "    df['OverallQC'] = (df['OverallQual'] + df['OverallCond'])/2\n",
    "    df['ExteriorQC'] = (df['ExterQual'] + df['ExterCond'])/2\n",
    "    df['GarageQC'] = (df['GarageQual'] + df['GarageCond'])/2\n",
    "\n",
    "    df['BsmtFinType1'] = df['BsmtFinType1'].map(lambda x: 1 if x=='GLQ' else 1 if x=='ALQ' else 0)\n",
    "    df['BsmtFinType2'] = df['BsmtFinType2'].map(lambda x: 1 if x=='GLQ' else 1 if x=='ALQ' else 0)\n",
    "\n",
    "    df['BsmtFinish'] = df['BsmtFinType1'] + df['BsmtFinType2']\n",
    "\n",
    "    df.drop(columns=['OverallQual','OverallCond','ExterQual','ExterCond','GarageQual','GarageCond','BsmtExposure','BsmtFinType1','BsmtFinType2'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fill_values(df):\n",
    "    df['Exterior1st'].fillna('VinylSd', inplace=True)\n",
    "    df['Exterior2nd'].fillna('VinylSd', inplace=True)\n",
    "    df['BsmtCond'].fillna(3, inplace=True)\n",
    "    df['BsmtFinSF1'].fillna(379.5, inplace=True)\n",
    "    df['BsmtFinSF2'].fillna(0, inplace=True)\n",
    "    df['BsmtUnfSF'].fillna(482.5, inplace=True)\n",
    "    df['TotalBsmtSF'].fillna(992, inplace=True)\n",
    "    df['BsmtFullBath'].fillna(0, inplace=True)\n",
    "    df['BsmtHalfBath'].fillna(0, inplace=True)\n",
    "    df['KitchenQual'].fillna(3, inplace=True)\n",
    "    df['GarageYrBlt'].fillna(1979, inplace=True)\n",
    "    df['GarageCars'].fillna(2, inplace=True)\n",
    "    df['GarageArea'].fillna(477.5, inplace=True)\n",
    "    df['SaleType'].fillna('WD', inplace=True)\n",
    "    df['BsmtHeight'].fillna(80, inplace=True)\n",
    "    df['GarageQC'].fillna(6, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng_tr(df):\n",
    "    \"\"\"Generates new features within the train dataset and provides a dictionary with average neighborhood decibel levels. The function returns the updated dataset along with a dictionary containing average decibel levels for each neighborhood.\"\"\"\n",
    "    # Total Porch Area\n",
    "    df['TotalPorchSF'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch']\n",
    "\n",
    "    # Total Living Area\n",
    "    df['TotalLivSF'] = df['MasVnrArea'] + df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GarageArea'] + df['PoolArea'] + df['TotalPorchSF'] + df['WoodDeckSF']\n",
    "\n",
    "    # New Built House\n",
    "    df['NewHouse'] = df['SaleType'].map(lambda x: 1 if x=='New' else 0)\n",
    "\n",
    "    # Expensive Neighborhood\n",
    "    df['ExpNeighborhood'] = df['Neighborhood'].map(lambda x: 1 if x=='NoRidge' else 1 if x=='NridgHt' else 1 if x=='StoneBr' else 0)\n",
    "\n",
    "    # Basement and Ground Bathrooms\n",
    "    df['BsmtHalfBath'] = df['BsmtHalfBath'].map(lambda x: 0.5 if x==1 else 1 if x==2 else 0)\n",
    "    df['HalfBath'] = df['HalfBath'].map(lambda x: 0.5 if x==1 else 1 if x==2 else 0)\n",
    "\n",
    "    df['BsmtBaths'] = df['BsmtFullBath'] + df['BsmtHalfBath']\n",
    "    df['GrBaths'] = df['FullBath'] + df['HalfBath']\n",
    "\n",
    "    # Age of the House\n",
    "    df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n",
    "\n",
    "    #Neighborhood Noise dB\n",
    "    condition = {'Norm':'Normal', 'Feedr':'Road', 'PosN':'Good', 'Artery':'Road', 'RRAe':'Railroad', 'RRNn':'Railroad', 'RRAn':'Railroad',\n",
    "                'PosA':'Good', 'RRNe':'Railroad'}\n",
    "    df['Condition1'] = df['Condition1'].replace(condition)\n",
    "    df['Condition2'] = df['Condition2'].replace(condition)\n",
    "    df['NeighborCondition'] = df['Condition1'] + df['Condition2']\n",
    "\n",
    "    condition2 = {'NormalNormal':60, 'RoadNormal':75, 'GoodNormal':55, 'RoadRoad':80,'RailroadNormal':85,\n",
    "    'RoadRailroad':100, 'RailroadRoad':100, 'GoodGood':50,'RoadGood':70}\n",
    "    df['NeighborNoise(dB)'] = df['NeighborCondition'].replace(condition2)\n",
    "\n",
    "    avgdB = df.groupby('Neighborhood')['NeighborNoise(dB)'].mean()\n",
    "    avg_dB_dict = avgdB.to_dict()\n",
    "\n",
    "    df['NeighborAvg_dB'] = df['Neighborhood'].replace(avg_dB_dict)\n",
    "    df['NeighborAvg_dB'] = round(df['NeighborAvg_dB'],2)\n",
    "\n",
    "    df.drop(columns=['MoSold','SaleType','SaleCondition','Condition1','Condition2','NeighborNoise(dB)','NeighborCondition','BsmtFullBath','BsmtHalfBath','YrSold','LotConfig','HalfBath'], inplace=True)\n",
    "    return df, avg_dB_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng_ts(df, dB_obj):\n",
    "    \"\"\"Generates new features within the test dataset and provides a dictionary with average neighborhood decibel levels. The function returns the updated dataset along with a dictionary containing average decibel levels for each neighborhood.\"\"\"\n",
    "    df['TotalPorchSF'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch']\n",
    "    df['TotalLivSF'] = df['MasVnrArea'] + df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GarageArea'] + df['PoolArea'] + df['TotalPorchSF'] + df['WoodDeckSF']\n",
    "    df['NewHouse'] = df['SaleType'].map(lambda x: 1 if x=='New' else 0)\n",
    "    df['ExpNeighborhood'] = df['Neighborhood'].map(lambda x: 1 if x=='NoRidge' else 1 if x=='NridgHt' else 1 if x=='StoneBr' else 0)\n",
    "    df['BsmtHalfBath'] = df['BsmtHalfBath'].map(lambda x: 0.5 if x==1 else 1 if x==2 else 0)\n",
    "    df['HalfBath'] = df['HalfBath'].map(lambda x: 0.5 if x==1 else 1 if x==2 else 0)\n",
    "    df['BsmtBaths'] = df['BsmtFullBath'] + df['BsmtHalfBath']\n",
    "    df['GrBaths'] = df['FullBath'] + df['HalfBath']\n",
    "    df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n",
    "    condition = {'Norm':'Normal', 'Feedr':'Road', 'PosN':'Good', 'Artery':'Road', 'RRAe':'Railroad', 'RRNn':'Railroad', 'RRAn':'Railroad',\n",
    "                'PosA':'Good', 'RRNe':'Railroad'}\n",
    "    df['Condition1'] = df['Condition1'].replace(condition)\n",
    "    df['Condition2'] = df['Condition2'].replace(condition)\n",
    "    df['NeighborCondition'] = df['Condition1'] + df['Condition2']\n",
    "\n",
    "    condition2 = {'NormalNormal':60, 'RoadNormal':75, 'GoodNormal':55, 'RoadRoad':80,'RailroadNormal':85,\n",
    "    'RoadRailroad':100, 'RailroadRoad':100, 'GoodGood':50,'RoadGood':70}\n",
    "    df['NeighborNoise(dB)'] = df['NeighborCondition'].replace(condition2)\n",
    "\n",
    "    avg_dB_dict = dB_obj\n",
    "\n",
    "    df['NeighborAvg_dB'] = df['Neighborhood'].replace(avg_dB_dict)\n",
    "    df['NeighborAvg_dB'] = round(df['NeighborAvg_dB'],2)\n",
    "\n",
    "    df.drop(columns=['MoSold','SaleType','SaleCondition','Condition1','Condition2','NeighborNoise(dB)','NeighborCondition','BsmtFullBath','BsmtHalfBath','YrSold','LotConfig','HalfBath'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_tr(df):\n",
    "    \"\"\"Performs One Hot Encoding on the train dataset and provides the encoded dataframe along with the encoder.\"\"\"\n",
    "\n",
    "    categorical_ft = df[df.select_dtypes('object').columns]\n",
    "    oneHot = ce.OneHotEncoder(use_cat_names=True)\n",
    "    oneHot.fit(categorical_ft)\n",
    "    encoded_train = oneHot.transform(categorical_ft)\n",
    "    return encoded_train, oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_ts(df, onehot_obj):\n",
    "    \"\"\"Performs One Hot Encoding on the test dataset.\"\"\"\n",
    "    categorical_ft = df[df.select_dtypes('object').columns]\n",
    "    encoded_test = onehot_obj.transform(categorical_ft)\n",
    "    return encoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_features_tr(df, encoded_df):\n",
    "    \"\"\"Returns selected features from the train dataset where the correlation with the target is greater than 0.1.\"\"\"\n",
    "\n",
    "    target_ft = df[['SalePrice']]\n",
    "    encoded_train_sub = encoded_df.join(target_ft)\n",
    "    encoded_corr = encoded_train_sub.corr()[['SalePrice']].reset_index().rename(columns={'index':'Feature', 'SalePrice':'Correlation'})\n",
    "    selected_corr = encoded_corr[abs(encoded_corr['Correlation'])>0.1]\n",
    "    selected_ft = np.delete(selected_corr['Feature'].values, [-1])\n",
    "    selected_encoded_df = encoded_df[selected_ft]\n",
    "    return selected_encoded_df, selected_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_features_ts(encoded_df, ft_obj):\n",
    "    \"\"\"Returns selected features from the test dataset.\"\"\"\n",
    "    selected_encoded_df = encoded_df[ft_obj]\n",
    "    return selected_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_tr(df):\n",
    "    \"\"\"Scales the train dataset using a standard scaler and returns the scaled dataframe along with the scaler.\"\"\"\n",
    "\n",
    "    df.drop(columns=['Street', 'Utilities'], inplace=True)\n",
    "    non_scaled = df[['YearBuilt', 'YearRemodAdd', 'BsmtCond', 'HeatingQC', 'CentralAir', 'FullBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'Functional',\n",
    "                    'Fireplaces', 'FireplaceQu', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'PavedDrive', 'OverallQC', 'ExteriorQC', 'GarageQC',\n",
    "                    'BsmtFinish', 'NewHouse', 'ExpNeighborhood', 'BsmtBaths', 'GrBaths']]\n",
    "    need_scale = df[df.select_dtypes(['float', 'int']).columns].drop(columns=non_scaled.columns)\n",
    "    need_scale.drop(columns='SalePrice', inplace=True)\n",
    "    std_SC = StandardScaler()\n",
    "    scaled_df = pd.DataFrame(std_SC.fit_transform(need_scale), columns=need_scale.columns)\n",
    "    return scaled_df, non_scaled, std_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_ts(df, scale_obj):\n",
    "    \"\"\"Scales the test dataset.\"\"\"\n",
    "    try:\n",
    "        df.drop(columns=['Street', 'Utilities','SalePrice'], inplace=True)\n",
    "        non_scaled = df[['YearBuilt', 'YearRemodAdd', 'BsmtCond', 'HeatingQC', 'CentralAir', 'FullBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'Functional',\n",
    "                            'Fireplaces', 'FireplaceQu', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'PavedDrive', 'OverallQC', 'ExteriorQC', 'GarageQC',\n",
    "                            'BsmtFinish', 'NewHouse', 'ExpNeighborhood', 'BsmtBaths', 'GrBaths']]\n",
    "\n",
    "        need_scale = df[df.select_dtypes(['float', 'int']).columns].drop(columns=non_scaled.columns)\n",
    "\n",
    "        scaled_df = pd.DataFrame(scale_obj.transform(need_scale), columns=need_scale.columns)\n",
    "    except:\n",
    "        df.drop(columns=['Street', 'Utilities'], inplace=True)\n",
    "        non_scaled = df[['YearBuilt', 'YearRemodAdd', 'BsmtCond', 'HeatingQC', 'CentralAir', 'FullBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'Functional',\n",
    "                            'Fireplaces', 'FireplaceQu', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'PavedDrive', 'OverallQC', 'ExteriorQC', 'GarageQC',\n",
    "                            'BsmtFinish', 'NewHouse', 'ExpNeighborhood', 'BsmtBaths', 'GrBaths']]\n",
    "\n",
    "        need_scale = df[df.select_dtypes(['float', 'int']).columns].drop(columns=non_scaled.columns)\n",
    "\n",
    "        scaled_df = pd.DataFrame(scale_obj.transform(need_scale), columns=need_scale.columns)\n",
    "    return scaled_df, non_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_train(df, encoded_df, scaled_df, non_scaled):\n",
    "    \"\"\"Provides the final train dataset prepared for model training.\"\"\"\n",
    "    target = df[['SalePrice']]\n",
    "    train_final = encoded_df.join(scaled_df)\n",
    "    train_final = train_final.join(non_scaled)\n",
    "    train_final = train_final.join(target)\n",
    "    return train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test(encoded_df, scaled_df, non_scaled):\n",
    "    \"\"\"Provides the final test dataset.\"\"\"\n",
    "    test_final = encoded_df.join(scaled_df)\n",
    "    test_final = test_final.join(non_scaled)\n",
    "    return test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(df):\n",
    "    \"\"\"Applies a logarithmic transformation to the sale price in the dataset.\"\"\"\n",
    "    df['SalePrice_trans'] = np.log(df['SalePrice'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_df(df):\n",
    "    \"\"\"Returns the x_train, x_test, y_train, and y_test dataframes after splitting the dataset into training and testing sets.\"\"\"\n",
    "    x = df.drop(columns=['SalePrice', 'SalePrice_trans'])\n",
    "    y = df['SalePrice_trans']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_data_split(df):\n",
    "    \"\"\"Splits the validation dataset into dependent and independent variables.\"\"\"\n",
    "    x = df.drop(columns=['SalePrice', 'SalePrice_trans'])\n",
    "    y = df['SalePrice_trans']\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(x_train, y_train):\n",
    "    \"\"\"Trains the following models:\n",
    "    - ElasticNet\n",
    "    - GradientBoostingRegressor\n",
    "    - XGBRegressor\"\"\"\n",
    "    en = ElasticNet(alpha = 0.001, l1_ratio=0.99)\n",
    "    gbr = GradientBoostingRegressor(n_estimators=500)\n",
    "    xgbr = xgb.XGBRegressor(n_estimators=2900, max_depth=2, learning_rate=0.01)\n",
    "\n",
    "    en.fit(x_train, y_train)\n",
    "    gbr.fit(x_train, y_train)\n",
    "    xgbr.fit(x_train, y_train)\n",
    "\n",
    "    return en, gbr, xgbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_prediction(x_test, en, gbr, xgbr):\n",
    "    \"\"\"Predicts house prices and returns predictions following a specified order.\n",
    "    - ElasticNet\n",
    "    - GradientBoostingRegressor\n",
    "    - XGBRegressor\n",
    "    \"\"\"\n",
    "    en_pred = en.predict(x_test)\n",
    "    gbr_pred = gbr.predict(x_test)\n",
    "    xgbr_pred = xgbr.predict(x_test)\n",
    "    return en_pred, gbr_pred, xgbr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_result(y_test, en_pred, gbr_pred, xgbr_pred):\n",
    "    \"\"\"Returns the Root Mean Squared Error (RMSE) dataframe of the predictions compared to the actual values, following a specific order.\"\"\"\n",
    "    result = pd.DataFrame({\n",
    "    \"ElasticNet\": {'RMSE':mean_squared_error(np.exp(y_test), np.exp(en_pred)) ** .5},\n",
    "    'GradientBoost': {'RMSE':mean_squared_error(np.exp(y_test), np.exp(gbr_pred)) ** .5},\n",
    "    \"XGBoost\": {'RMSE':mean_squared_error(np.exp(y_test), np.exp(xgbr_pred)) ** .5},\n",
    "    })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pred_tr(y_test, en_pred, gbr_pred, xgbr_pred):\n",
    "    \"\"\"Returns the weighted prediction calculated from the training dataset.\"\"\"\n",
    "    weighted_pred = en_pred * 0.5 + gbr_pred * 0.2 + xgbr_pred * 0.3\n",
    "    y_test_inv = np.exp(y_test)\n",
    "    return weighted_pred, y_test_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pred_ts(en_pred, gbr_pred, xgbr_pred):\n",
    "    \"\"\"Returns the weighted prediction calculated from the test dataset.\"\"\"\n",
    "    weighted_pred = en_pred * 0.5 + gbr_pred * 0.2 + xgbr_pred * 0.3\n",
    "    result = pd.DataFrame(np.exp(weighted_pred), columns=['SalePrice'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weighte_pred_rmse(weighted_pred, y_test_inv):\n",
    "    \"\"\"Prints the Root Mean Squared Error (RMSE) of the weighted prediction calculated from the provided dataset.\"\"\"\n",
    "    print('Weighted RMSE:',mean_squared_error(y_test_inv, np.exp(weighted_pred)) ** .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_objects(obj, str_name):\n",
    "    \"\"\"Saves the object using pickle serialization with the specified filename.\"\"\"\n",
    "    pickle.dump(obj, open(str_name + \".pickle\", \"wb\"))\n",
    "    print(\"Pickle save complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_objects(path):\n",
    "    \"\"\"Loads the object previously saved using pickle serialization from the specified filename.\"\"\"\n",
    "    obj = pickle.load(open(path, \"rb\"))\n",
    "    return obj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
